name: CI Enhanced with Metrics

on:
  push:
    branches:
      - main
      - staging/*
      - next_stable
      - '20*'
  pull_request:
    branches:
      - main
      - next_stable
      - '20*'

env:
  NUM_JOBS: 4
  # Set to 'true' to fail Static Analysis job when warnings/style/performance issues are found
  # Set to 'false' to only fail on errors (warnings become informational)
  STATIC_ANALYSIS_STRICT: 'false'

jobs:
  # Code style checking with astyle
  astyle:
    name: Code Style Check
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 50
        ref: ${{ github.event.pull_request.head.sha || github.sha }}

    - name: Fetch target branch
      run: |
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          git fetch --depth=50 origin ${{ github.event.pull_request.base.ref }}
        else
          git fetch --depth=50 origin main
        fi

    - name: Install astyle
      run: |
        sudo apt-get update
        sudo apt-get install -y astyle

    - name: Set environment variables
      run: |
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          echo "TARGET_BRANCH=${{ github.event.pull_request.base.ref }}" >> $GITHUB_ENV
        else
          echo "TARGET_BRANCH=main" >> $GITHUB_ENV
        fi
        echo "BUILD_TYPE=astyle" >> $GITHUB_ENV

    - name: Run astyle check
      run: |
        # Capture astyle output and git state
        echo "=== Running astyle check ===" > astyle_output.txt

        # Count source files before running astyle
        TOTAL_SOURCE_FILES=$(find . -name "*.c" -o -name "*.h" 2>/dev/null | grep -v ".git" | grep -v "build/" | wc -l)
        echo "Total source files found: $TOTAL_SOURCE_FILES" >> astyle_output.txt

        # Run astyle and capture detailed output
        if ./ci/run_build.sh >> astyle_output.txt 2>&1; then
          echo "astyle_failed=false" >> $GITHUB_ENV
          echo "Astyle check PASSED" >> astyle_output.txt
        else
          echo "astyle_failed=true" >> $GITHUB_ENV
          echo "Astyle check FAILED" >> astyle_output.txt
        fi

        # Check for files that were modified by astyle
        MODIFIED_FILES=$(git diff --name-only 2>/dev/null | grep -E "\.(c|h)$" | wc -l || echo "0")
        echo "Files modified by astyle: $MODIFIED_FILES" >> astyle_output.txt

        # Show some example changes if any
        if [ "$MODIFIED_FILES" -gt 0 ]; then
          echo "=== Modified files ===" >> astyle_output.txt
          git diff --name-only | grep -E "\.(c|h)$" >> astyle_output.txt 2>/dev/null || true
        fi

    - name: Parse astyle results
      run: |
        # Parse astyle results from our detailed output
        if [ -f "astyle_output.txt" ]; then
          # Get total source files
          TOTAL_FILES=$(grep "Total source files found:" astyle_output.txt | grep -o '[0-9]*' || echo "0")

          # Get files modified by astyle
          FILES_WITH_ISSUES=$(grep "Files modified by astyle:" astyle_output.txt | grep -o '[0-9]*' || echo "0")

          # Fallback: count source files if not captured
          if [ "$TOTAL_FILES" = "0" ] || [ -z "$TOTAL_FILES" ]; then
            TOTAL_FILES=$(find . -name "*.c" -o -name "*.h" 2>/dev/null | grep -v ".git" | grep -v "build/" | wc -l || echo "0")
          fi

          # Fallback: check git diff directly
          if [ "$FILES_WITH_ISSUES" = "0" ] || [ -z "$FILES_WITH_ISSUES" ]; then
            FILES_WITH_ISSUES=$(git diff --name-only 2>/dev/null | grep -E "\.(c|h)$" | wc -l || echo "0")
          fi
        else
          # If no output file, count files manually
          TOTAL_FILES=$(find . -name "*.c" -o -name "*.h" 2>/dev/null | grep -v ".git" | grep -v "build/" | wc -l || echo "0")
          FILES_WITH_ISSUES=$(git diff --name-only 2>/dev/null | grep -E "\.(c|h)$" | wc -l || echo "0")
        fi

        # Clean the variables
        TOTAL_FILES=$(echo "$TOTAL_FILES" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")
        FILES_WITH_ISSUES=$(echo "$FILES_WITH_ISSUES" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")

        # Ensure valid numbers
        TOTAL_FILES=${TOTAL_FILES:-0}
        FILES_WITH_ISSUES=${FILES_WITH_ISSUES:-0}

        # Validate numbers are reasonable
        if [ "$TOTAL_FILES" -lt 0 ] || [ "$TOTAL_FILES" -gt 100000 ]; then
          TOTAL_FILES=0
        fi
        if [ "$FILES_WITH_ISSUES" -lt 0 ] || [ "$FILES_WITH_ISSUES" -gt "$TOTAL_FILES" ]; then
          FILES_WITH_ISSUES=0
        fi

        # Calculate files OK
        FILES_OK=$((TOTAL_FILES - FILES_WITH_ISSUES))

        echo "TOTAL_FILES=$TOTAL_FILES" >> $GITHUB_ENV
        echo "FILES_WITH_ISSUES=$FILES_WITH_ISSUES" >> $GITHUB_ENV
        echo "FILES_OK=$FILES_OK" >> $GITHUB_ENV

        # Debug output
        echo "Debug: TOTAL_FILES=$TOTAL_FILES, FILES_WITH_ISSUES=$FILES_WITH_ISSUES, FILES_OK=$FILES_OK"

    - name: Create Job Summary
      if: always()
      run: |
        # Calculate compliance rate safely
        TOTAL_FILES="${{ env.TOTAL_FILES }}"
        FILES_OK="${{ env.FILES_OK }}"
        if [ "${TOTAL_FILES:-0}" -eq 0 ]; then
          COMPLIANCE_RATE="N/A"
        else
          COMPLIANCE_RATE=$(( FILES_OK * 100 / TOTAL_FILES ))
        fi

        cat >> $GITHUB_STEP_SUMMARY << EOF
        ## [üé® Code Style Check Results](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}/job/${{ jobs.job_id }})

        | Metric | Value |
        |--------|-------|
        | üìÅ Total Files Checked | ${{ env.TOTAL_FILES }} |
        | ‚úÖ Files Compliant | ${{ env.FILES_OK }} |
        | ‚ùå Files with Issues | ${{ env.FILES_WITH_ISSUES }} |
        | üìä Compliance Rate | ${COMPLIANCE_RATE}% |

        ### Status: ${{ env.astyle_failed == 'true' && '‚ùå Failed' || '‚úÖ Passed' }}

        > üí° **Tip**: Click the heading above to view the workflow run page
        EOF

  # Static analysis with cppcheck
  cppcheck:
    name: Static Analysis
    runs-on: ubuntu-latest
    outputs:
      total_issues: ${{ env.CPPCHECK_TOTAL }}
      has_errors: ${{ env.CPPCHECK_ERRORS }}
      has_warnings: ${{ env.CPPCHECK_WARNINGS }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 50
        ref: ${{ github.event.pull_request.head.sha || github.sha }}

    - name: Fetch target branch
      run: |
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          git fetch --depth=50 origin ${{ github.event.pull_request.base.ref }}
        else
          git fetch --depth=50 origin main
        fi

    - name: Install cppcheck
      run: |
        sudo apt-get update
        sudo apt-get install -y cppcheck

    - name: Set environment variables
      run: |
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          echo "TARGET_BRANCH=${{ github.event.pull_request.base.ref }}" >> $GITHUB_ENV
        else
          echo "TARGET_BRANCH=main" >> $GITHUB_ENV
        fi
        echo "BUILD_TYPE=cppcheck" >> $GITHUB_ENV

    - name: Run cppcheck analysis
      run: |
        set -o pipefail
        
        # Run cppcheck and capture the exit code
        if ./ci/run_build.sh 2>&1 | tee cppcheck_output.txt; then
          echo "cppcheck_completed=true" >> $GITHUB_ENV
          echo "cppcheck_failed=false" >> $GITHUB_ENV
        else
          EXIT_CODE=$?
          echo "cppcheck_completed=true" >> $GITHUB_ENV
          # Only mark as failed if it's a real failure (not just finding issues)
          # cppcheck exits with code 1 when issues are found, which is normal
          if [ $EXIT_CODE -gt 1 ]; then
            echo "cppcheck_failed=true" >> $GITHUB_ENV
          else
            echo "cppcheck_failed=false" >> $GITHUB_ENV
          fi
        fi

    - name: Parse cppcheck results
      run: |
        # Parse different types of issues with safe parsing
        if [ -f "cppcheck_output.txt" ]; then
          ERRORS=$(grep -c ": error:" cppcheck_output.txt 2>/dev/null || echo "0")
          WARNINGS=$(grep -c ": warning:" cppcheck_output.txt 2>/dev/null || echo "0")
          STYLE=$(grep -c ": style:" cppcheck_output.txt 2>/dev/null || echo "0")
          PERFORMANCE=$(grep -c ": performance:" cppcheck_output.txt 2>/dev/null || echo "0")
        else
          ERRORS="0"
          WARNINGS="0"
          STYLE="0"
          PERFORMANCE="0"
        fi

        # Clean the variables of any whitespace/newlines
        ERRORS=$(echo "$ERRORS" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")
        WARNINGS=$(echo "$WARNINGS" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")
        STYLE=$(echo "$STYLE" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")
        PERFORMANCE=$(echo "$PERFORMANCE" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")

        # Ensure we have valid numbers
        ERRORS=${ERRORS:-0}
        WARNINGS=${WARNINGS:-0}
        STYLE=${STYLE:-0}
        PERFORMANCE=${PERFORMANCE:-0}

        # Validate numbers are reasonable
        if [ "$ERRORS" -lt 0 ] || [ "$ERRORS" -gt 10000 ]; then
          ERRORS=0
        fi
        if [ "$WARNINGS" -lt 0 ] || [ "$WARNINGS" -gt 10000 ]; then
          WARNINGS=0
        fi
        if [ "$STYLE" -lt 0 ] || [ "$STYLE" -gt 10000 ]; then
          STYLE=0
        fi
        if [ "$PERFORMANCE" -lt 0 ] || [ "$PERFORMANCE" -gt 10000 ]; then
          PERFORMANCE=0
        fi

        # Calculate total safely
        TOTAL_ISSUES=$((ERRORS + WARNINGS + STYLE + PERFORMANCE))

        echo "CPPCHECK_ERRORS=$ERRORS" >> $GITHUB_ENV
        echo "CPPCHECK_WARNINGS=$WARNINGS" >> $GITHUB_ENV
        echo "CPPCHECK_STYLE=$STYLE" >> $GITHUB_ENV
        echo "CPPCHECK_PERFORMANCE=$PERFORMANCE" >> $GITHUB_ENV
        echo "CPPCHECK_TOTAL=$TOTAL_ISSUES" >> $GITHUB_ENV

        # Debug output
        echo "Debug: ERRORS=$ERRORS, WARNINGS=$WARNINGS, STYLE=$STYLE, PERFORMANCE=$PERFORMANCE, TOTAL=$TOTAL_ISSUES"

    - name: Create Job Summary
      if: always()
      run: |
        # Calculate status message
        # Convert string values to numbers for comparison
        ERRORS=$((0 + ${{ env.CPPCHECK_ERRORS }}))
        TOTAL=$((0 + ${{ env.CPPCHECK_TOTAL }}))
        
        # Determine status based on results
        if [ "${{ env.cppcheck_failed }}" = "true" ]; then
          STATUS_MSG="‚ùå Failed"
        elif [ $ERRORS -gt 0 ]; then
          STATUS_MSG="‚ö†Ô∏è Has Errors"
        elif [ $TOTAL -gt 0 ]; then
          STATUS_MSG="‚ö†Ô∏è Has Warnings"
        else
          STATUS_MSG="‚úÖ Passed"
        fi

        cat >> $GITHUB_STEP_SUMMARY << EOF
        ## [üîç Static Analysis Results](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

        | Issue Type | Count |
        |------------|-------|
        | üö® Errors | ${{ env.CPPCHECK_ERRORS }} |
        | ‚ö†Ô∏è Warnings | ${{ env.CPPCHECK_WARNINGS }} |
        | üéØ Style Issues | ${{ env.CPPCHECK_STYLE }} |
        | ‚ö° Performance Issues | ${{ env.CPPCHECK_PERFORMANCE }} |
        | **üìä Total Issues** | **${{ env.CPPCHECK_TOTAL }}** |

        ### Status: ${STATUS_MSG}

        > üí° **Tip**: Click the heading above to view the workflow run page
        EOF

        # Add issue breakdown if there are issues
        if [ $TOTAL -gt 0 ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### üìã Issue Breakdown" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Errors**: High priority issues that should be fixed immediately" >> $GITHUB_STEP_SUMMARY
          echo "- **Warnings**: Potential problems worth investigating" >> $GITHUB_STEP_SUMMARY
          echo "- **Style**: Code style improvements for better readability" >> $GITHUB_STEP_SUMMARY
          echo "- **Performance**: Optimization suggestions for better efficiency" >> $GITHUB_STEP_SUMMARY

          # Show actual issues if cppcheck output exists
          WARNINGS=$((0 + ${{ env.CPPCHECK_WARNINGS }}))
          if [ -f "cppcheck_output.txt" ] && [ $WARNINGS -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "#### ‚ö†Ô∏è Warning Details" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            grep ": warning:" cppcheck_output.txt | head -10 >> $GITHUB_STEP_SUMMARY 2>/dev/null || \
              echo "Warning details not available" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f "cppcheck_output.txt" ] && [ $ERRORS -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "#### üö® Error Details" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            grep ": error:" cppcheck_output.txt | head -10 >> $GITHUB_STEP_SUMMARY 2>/dev/null || \
              echo "Error details not available" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üéâ **No issues found!** Your code passes all static analysis checks." >> $GITHUB_STEP_SUMMARY
        fi

    - name: Fail job if issues found
      run: |
        echo "Static Analysis Mode: ${{ env.STATIC_ANALYSIS_STRICT == 'true' && 'STRICT' || 'LENIENT' }}"
        echo "Issues found - Errors: ${{ env.CPPCHECK_ERRORS }}, Warnings: ${{ env.CPPCHECK_WARNINGS }}, Style: ${{ env.CPPCHECK_STYLE }}, Performance: ${{ env.CPPCHECK_PERFORMANCE }}"

        # Convert string values to numbers for comparison
        ERRORS=$((0 + ${{ env.CPPCHECK_ERRORS }}))
        TOTAL=$((0 + ${{ env.CPPCHECK_TOTAL }}))

        if [ "${{ env.STATIC_ANALYSIS_STRICT }}" = "true" ]; then
          # Strict mode: fail on any issues (errors, warnings, style, performance)
          if [ $TOTAL -gt 0 ]; then
            echo "‚ùå STRICT MODE: Static analysis found $TOTAL issues that need to be addressed."
            echo "Set STATIC_ANALYSIS_STRICT=false to make warnings/style/performance issues non-blocking."
            exit 1
          else
            echo "‚úÖ STRICT MODE: No static analysis issues found."
          fi
        else
          # Lenient mode: only fail on errors (warnings/style/performance are informational)
          if [ $ERRORS -gt 0 ]; then
            echo "‚ùå LENIENT MODE: Static analysis found $ERRORS errors that must be fixed."
            exit 1
          else
            echo "‚úÖ LENIENT MODE: No errors found. Warnings/style/performance issues are informational."
            if [ $TOTAL -gt 0 ]; then
              echo "‚ÑπÔ∏è  Found $TOTAL non-blocking issues (warnings/style/performance)."
            fi
          fi
        fi

  # Build drivers with metrics
  drivers:
    name: Build Drivers
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 50

    - name: Install build dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential gcc-arm-none-eabi

    - name: Set environment variables
      run: |
        echo "BUILD_TYPE=drivers" >> $GITHUB_ENV

    - name: Build drivers with metrics
      run: |
        START_TIME=$(date +%s)

        # Capture build output with detailed logging
        echo "=== Starting drivers build ===" > build_output.txt
        echo "Build started at: $(date)" >> build_output.txt

        # Count actual driver source files that will be compiled
        # Use the same logic as the drivers Makefile
        cd drivers
        SKIPDIR="-path ./platform -prune -o -path ./axi_core -prune -o -path ./rf-transceiver/navassa -prune -o -path ./adc/ad9081 -prune -o -path ./rf-transceiver/madura -prune -o -path ./net/oa_tc6 -prune -o"
        TOTAL_DRIVERS=$(find . $SKIPDIR -name '*.c' -print | wc -l || echo "0")
        cd ..

        echo "Total driver files to compile: $TOTAL_DRIVERS" >> build_output.txt
        echo "This matches the drivers/Makefile SRCS variable" >> build_output.txt

        # Run the build and capture result
        if ./ci/run_build.sh >> build_output.txt 2>&1; then
          BUILD_RESULT=0
          echo "BUILD_STATUS=SUCCESS" >> build_output.txt
          echo "All driver compilation completed successfully" >> build_output.txt
        else
          BUILD_RESULT=1
          echo "BUILD_STATUS=FAILED" >> build_output.txt
          echo "Driver compilation failed with errors" >> build_output.txt
        fi

        END_TIME=$(date +%s)
        BUILD_TIME=$((END_TIME - START_TIME))

        echo "Build completed at: $(date)" >> build_output.txt
        echo "Total build time: ${BUILD_TIME}s" >> build_output.txt
        echo "Driver files processed: $TOTAL_DRIVERS" >> build_output.txt

        echo "BUILD_TIME=$BUILD_TIME" >> $GITHUB_ENV
        echo "BUILD_RESULT=$BUILD_RESULT" >> $GITHUB_ENV

    - name: Parse build results
      run: |
        # Parse build results from our detailed output
        if [ -f "build_output.txt" ]; then
          # Get total driver files that were compiled
          TOTAL_DRIVERS=$(grep "Total driver files to compile:" build_output.txt | grep -o '[0-9]*' || echo "0")

          # Get build status
          BUILD_STATUS=$(grep "BUILD_STATUS=" build_output.txt | cut -d'=' -f2 || echo "UNKNOWN")

          # Determine successful vs failed builds based on overall result
          if [ "$BUILD_STATUS" = "SUCCESS" ] || [ "${{ env.BUILD_RESULT }}" = "0" ]; then
            SUCCESSFUL_BUILDS="$TOTAL_DRIVERS"
            FAILED_BUILDS="0"
          elif [ "$BUILD_STATUS" = "FAILED" ] || [ "${{ env.BUILD_RESULT }}" != "0" ]; then
            SUCCESSFUL_BUILDS="0"
            FAILED_BUILDS="$TOTAL_DRIVERS"
          else
            # Unknown status - be conservative
            SUCCESSFUL_BUILDS="0"
            FAILED_BUILDS="0"
          fi

          # Fallback: count driver files if not captured
          if [ "$TOTAL_DRIVERS" = "0" ] || [ -z "$TOTAL_DRIVERS" ]; then
            cd drivers 2>/dev/null || cd .
            SKIPDIR="-path ./platform -prune -o -path ./axi_core -prune -o -path ./rf-transceiver/navassa -prune -o -path ./adc/ad9081 -prune -o -path ./rf-transceiver/madura -prune -o -path ./net/oa_tc6 -prune -o"
            TOTAL_DRIVERS=$(find . $SKIPDIR -name '*.c' -print 2>/dev/null | wc -l || echo "0")
            cd .. 2>/dev/null || true
          fi
        else
          # If no output file, count driver files manually
          cd drivers 2>/dev/null || cd .
          SKIPDIR="-path ./platform -prune -o -path ./axi_core -prune -o -path ./rf-transceiver/navassa -prune -o -path ./adc/ad9081 -prune -o -path ./rf-transceiver/madura -prune -o -path ./net/oa_tc6 -prune -o"
          TOTAL_DRIVERS=$(find . $SKIPDIR -name '*.c' -print 2>/dev/null | wc -l || echo "0")
          cd .. 2>/dev/null || true

          # Use build result from environment
          if [ "${{ env.BUILD_RESULT }}" = "0" ]; then
            SUCCESSFUL_BUILDS="$TOTAL_DRIVERS"
            FAILED_BUILDS="0"
          else
            SUCCESSFUL_BUILDS="0"
            FAILED_BUILDS="$TOTAL_DRIVERS"
          fi
        fi

        # Clean the variables
        TOTAL_DRIVERS=$(echo "$TOTAL_DRIVERS" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")
        SUCCESSFUL_BUILDS=$(echo "$SUCCESSFUL_BUILDS" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")
        FAILED_BUILDS=$(echo "$FAILED_BUILDS" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")

        # Ensure valid numbers
        TOTAL_DRIVERS=${TOTAL_DRIVERS:-0}
        SUCCESSFUL_BUILDS=${SUCCESSFUL_BUILDS:-0}
        FAILED_BUILDS=${FAILED_BUILDS:-0}

        # Validate numbers are reasonable
        if [ "$TOTAL_DRIVERS" -lt 0 ] || [ "$TOTAL_DRIVERS" -gt 1000 ]; then
          TOTAL_DRIVERS=0
        fi
        if [ "$SUCCESSFUL_BUILDS" -lt 0 ] || [ "$SUCCESSFUL_BUILDS" -gt "$TOTAL_DRIVERS" ]; then
          SUCCESSFUL_BUILDS=0
        fi
        if [ "$FAILED_BUILDS" -lt 0 ] || [ "$FAILED_BUILDS" -gt "$TOTAL_DRIVERS" ]; then
          FAILED_BUILDS=0
        fi

        echo "SUCCESSFUL_BUILDS=$SUCCESSFUL_BUILDS" >> $GITHUB_ENV
        echo "FAILED_BUILDS=$FAILED_BUILDS" >> $GITHUB_ENV
        echo "TOTAL_DRIVERS=$TOTAL_DRIVERS" >> $GITHUB_ENV

        # Debug output
        echo "Debug: TOTAL_DRIVERS=$TOTAL_DRIVERS, SUCCESSFUL_BUILDS=$SUCCESSFUL_BUILDS, FAILED_BUILDS=$FAILED_BUILDS, BUILD_STATUS=$BUILD_STATUS"

    - name: Create Job Summary
      if: always()
      run: |
        # Calculate success rate safely
        TOTAL_DRIVERS="${{ env.TOTAL_DRIVERS }}"
        SUCCESSFUL_BUILDS="${{ env.SUCCESSFUL_BUILDS }}"
        if [ "${TOTAL_DRIVERS:-0}" -eq 0 ]; then
          SUCCESS_RATE="N/A"
        else
          SUCCESS_RATE=$(( SUCCESSFUL_BUILDS * 100 / TOTAL_DRIVERS ))
        fi

        cat >> $GITHUB_STEP_SUMMARY << EOF
        ## [üî® Drivers Build Results](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

        ### üìä Build Metrics
        | Metric | Value |
        |--------|-------|
        | ‚è±Ô∏è Build Time | ${{ env.BUILD_TIME }}s |
        | üéØ Total Driver Files | ${{ env.TOTAL_DRIVERS }} |
        | ‚úÖ Successfully Compiled | ${{ env.SUCCESSFUL_BUILDS }} |
        | ‚ùå Failed to Compile | ${{ env.FAILED_BUILDS }} |
        | üìà Success Rate | ${SUCCESS_RATE}% |

        ### Status: ${{ env.BUILD_RESULT == '0' && '‚úÖ All Drivers Compiled Successfully' || '‚ùå Driver Compilation Failed' }}

        ${{ env.BUILD_TIME > 300 && '‚ö†Ô∏è **Build time exceeded 5 minutes. Consider optimization.**' || '' }}

        > üí° **Note**: This builds reusable driver components (${TOTAL_DRIVERS} .c files), not the 134 application projects in the \`projects/\` directory.
        EOF

  # Unit tests with coverage metrics
  unit-tests:
    name: Unit Tests & Coverage
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 50

    - name: Setup Ruby
      uses: ruby/setup-ruby@v1
      with:
        ruby-version: '3.0'
        bundler-cache: false

    - name: Install Ceedling and dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential gcc lcov python3-pip
        pip3 install gcovr lxml
        gem install ceedling

    - name: Run unit tests and generate coverage
      run: |
        set -e

        # Initialize counters
        TOTAL_TESTS=0
        PASSED_TESTS=0
        FAILED_TESTS=0
        TEST_DIRS_COUNT=0

        # Coverage tracking
        declare -A COVERAGE_DATA
        OVERALL_LINE_COV=0
        OVERALL_BRANCH_COV=0
        OVERALL_FUNC_COV=0

        # Create detailed test results file
        echo "=== Unit Test Results ===" > test_results.txt
        echo "Run started at: $(date)" >> test_results.txt
        echo "" >> test_results.txt

        # Find all test directories containing project.yml files
        test_dirs=$(find tests -name "project.yml" -type f -exec dirname {} \;)

        if [ -z "$test_dirs" ]; then
          echo "No test directories found with project.yml files"
          exit 1
        fi

        for test_dir in $test_dirs; do
          echo "=== Running tests in: $test_dir ===" >> test_results.txt
          echo "Running tests in: $test_dir"
          cd "$test_dir"

          # Run tests with coverage
          if ceedling gcov:all > test_output.txt 2>&1; then
            echo "‚úÖ Test suite completed successfully" >> ../../../test_results.txt
          else
            echo "‚ùå Test suite encountered errors" >> ../../../test_results.txt
          fi

          # Parse test results from Ceedling output
          DIR_TESTS=$(grep "^TESTED:" test_output.txt 2>/dev/null | tail -1 | grep -o '[0-9]\+' || echo "0")
          DIR_PASSED=$(grep "^PASSED:" test_output.txt 2>/dev/null | tail -1 | grep -o '[0-9]\+' || echo "0")
          DIR_FAILED=$(grep "^FAILED:" test_output.txt 2>/dev/null | tail -1 | grep -o '[0-9]\+' || echo "0")

          # Parse coverage data from console output
          # Extract line coverage percentages
          LINE_COV=$(grep -E "Lines executed:[0-9.]+%" test_output.txt | sed 's/.*Lines executed:\([0-9.]\+\)%.*/\1/' | head -1 || echo "0")
          BRANCH_COV=$(grep -E "Branches executed:[0-9.]+%" test_output.txt | sed 's/.*Branches executed:\([0-9.]\+\)%.*/\1/' | head -1 || echo "0")

          # Parse HTML coverage report if available
          if [ -f "build/artifacts/gcov/gcovr/GcovCoverageResults.html" ]; then
            # Extract overall coverage from HTML
            HTML_LINE_COV=$(grep -o 'class="coverage-[^"]*">[0-9.]\+%' build/artifacts/gcov/gcovr/GcovCoverageResults.html | grep -o '[0-9.]\+' | head -1 || echo "0")
            if [ -n "$HTML_LINE_COV" ] && [ "$HTML_LINE_COV" != "0" ]; then
              LINE_COV=$HTML_LINE_COV
            fi
          fi

          # Store coverage data
          COVERAGE_DATA["$test_dir"]="$LINE_COV"

          # Validate the numbers
          DIR_TESTS=${DIR_TESTS:-0}
          DIR_PASSED=${DIR_PASSED:-0}
          DIR_FAILED=${DIR_FAILED:-0}
          LINE_COV=${LINE_COV:-0}

          # Capture failed test names
          if [ "$DIR_FAILED" -gt 0 ]; then
            echo "‚ùå Failed tests in $test_dir:" >> ../../../test_results.txt
            grep -A 5 -B 2 "FAIL\|ERROR" test_output.txt | head -20 >> ../../../test_results.txt 2>/dev/null || \
              echo "  (Failed test details not available)" >> ../../../test_results.txt
            echo "" >> ../../../test_results.txt
          else
            echo "‚úÖ All tests passed in $test_dir" >> ../../../test_results.txt
          fi

          # Add coverage info to results
          echo "üìä Coverage: ${LINE_COV}% lines" >> ../../../test_results.txt
          echo "Total tests in $test_dir: $DIR_TESTS (Passed: $DIR_PASSED, Failed: $DIR_FAILED)" >> ../../../test_results.txt
          echo "" >> ../../../test_results.txt

          TOTAL_TESTS=$((TOTAL_TESTS + DIR_TESTS))
          PASSED_TESTS=$((PASSED_TESTS + DIR_PASSED))
          FAILED_TESTS=$((FAILED_TESTS + DIR_FAILED))
          TEST_DIRS_COUNT=$((TEST_DIRS_COUNT + 1))

          # Accumulate coverage (simple average for now)
          OVERALL_LINE_COV=$(echo "$OVERALL_LINE_COV + $LINE_COV" | bc -l 2>/dev/null || echo "$OVERALL_LINE_COV")

          cd - > /dev/null
        done

        # Calculate average coverage
        if [ "$TEST_DIRS_COUNT" -gt 0 ]; then
          OVERALL_LINE_COV=$(echo "scale=1; $OVERALL_LINE_COV / $TEST_DIRS_COUNT" | bc -l 2>/dev/null || echo "0")
        fi

        # Add summary to results
        echo "=== Test Summary ===" >> test_results.txt
        echo "Total test suites: $TEST_DIRS_COUNT" >> test_results.txt
        echo "Total tests: $TOTAL_TESTS" >> test_results.txt
        echo "Passed: $PASSED_TESTS" >> test_results.txt
        echo "Failed: $FAILED_TESTS" >> test_results.txt
        echo "Overall Line Coverage: ${OVERALL_LINE_COV}%" >> test_results.txt

        # Calculate test success rate
        if [ $TOTAL_TESTS -gt 0 ]; then
          TEST_SUCCESS_RATE=$((PASSED_TESTS * 100 / TOTAL_TESTS))
        else
          TEST_SUCCESS_RATE=0
        fi

        # Set environment variables for GitHub Actions
        echo "TOTAL_TESTS=$TOTAL_TESTS" >> $GITHUB_ENV
        echo "PASSED_TESTS=$PASSED_TESTS" >> $GITHUB_ENV
        echo "FAILED_TESTS=$FAILED_TESTS" >> $GITHUB_ENV
        echo "TEST_SUCCESS_RATE=$TEST_SUCCESS_RATE" >> $GITHUB_ENV
        echo "TEST_DIRS_COUNT=$TEST_DIRS_COUNT" >> $GITHUB_ENV
        echo "OVERALL_LINE_COV=$OVERALL_LINE_COV" >> $GITHUB_ENV

    - name: Create Enhanced Job Summary
      if: always()
      run: |
        # Determine coverage status emoji and color
        COV_EMOJI="üî¥"
        COV_STATUS="Low"
        if (( $(echo "${{ env.OVERALL_LINE_COV }} >= 90" | bc -l) )); then
          COV_EMOJI="üü¢"
          COV_STATUS="High"
        elif (( $(echo "${{ env.OVERALL_LINE_COV }} >= 75" | bc -l) )); then
          COV_EMOJI="üü°"
          COV_STATUS="Medium"
        fi

        cat >> $GITHUB_STEP_SUMMARY << 'EOF'
        ## üß™ Unit Tests & Coverage Dashboard

        ### üìä Test Metrics
        | Metric | Value | Status |
        |--------|-------|--------|
        | üéØ Test Suites | ${{ env.TEST_DIRS_COUNT }} | ‚úÖ |
        | ‚úÖ Total Tests | ${{ env.TOTAL_TESTS }} | ‚úÖ |
        | üü¢ Passed | ${{ env.PASSED_TESTS }} | ‚úÖ |
        | üî¥ Failed | ${{ env.FAILED_TESTS }} | ${{ env.FAILED_TESTS == '0' && '‚úÖ' || '‚ùå' }} |
        | üìà Success Rate | ${{ env.TEST_SUCCESS_RATE }}% | ${{ env.TEST_SUCCESS_RATE >= 95 && '‚úÖ' || env.TEST_SUCCESS_RATE >= 90 && 'üü°' || '‚ùå' }} |

        ### üéØ Coverage Metrics
        | Type | Coverage | Status |
        |------|----------|--------|
        | üìä Line Coverage | ${{ env.OVERALL_LINE_COV }}% | ${COV_EMOJI} ${COV_STATUS} |

        ### Overall Status: ${{ env.FAILED_TESTS == '0' && '‚úÖ All Tests Passed' || '‚ùå Test Failures Detected' }}

        ${{ env.TEST_SUCCESS_RATE < 90 && '‚ö†Ô∏è **Test success rate below 90%. Consider improving test reliability.**' || '' }}
        ${{ env.TOTAL_TESTS == '0' && '‚ö†Ô∏è **No tests found. Consider adding unit tests.**' || '' }}

        EOF

        # Add coverage recommendations
        if (( $(echo "${{ env.OVERALL_LINE_COV }} < 75" | bc -l) )); then
          echo "‚ö†Ô∏è **Line coverage is below 75%. Consider adding more comprehensive tests.**" >> $GITHUB_STEP_SUMMARY
        fi

        # Add detailed test results if there are failures
        if [ "${{ env.FAILED_TESTS }}" != "0" ] && [ -f "test_results.txt" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üîç Detailed Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          grep -A 20 "Failed tests" test_results.txt | head -50 >> $GITHUB_STEP_SUMMARY 2>/dev/null || \
            echo "Test failure details not available" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "> üìÅ **Full test results are available in the uploaded artifacts**" >> $GITHUB_STEP_SUMMARY
        fi

    - name: Generate Coverage Badge Data
      if: always()
      run: |
        # Create badge data for potential use
        COV_COLOR="red"
        if (( $(echo "${{ env.OVERALL_LINE_COV }} >= 90" | bc -l) )); then
          COV_COLOR="brightgreen"
        elif (( $(echo "${{ env.OVERALL_LINE_COV }} >= 75" | bc -l) )); then
          COV_COLOR="yellow"
        fi

        # Create a simple JSON with coverage data
        cat > coverage-badge.json << EOF
        {
          "schemaVersion": 1,
          "label": "coverage",
          "message": "${{ env.OVERALL_LINE_COV }}%",
          "color": "$COV_COLOR"
        }
        EOF

        echo "Coverage badge data generated"

    - name: Check Coverage Threshold
      if: always()
      run: |
        THRESHOLD=70
        if (( $(echo "${{ env.OVERALL_LINE_COV }} < $THRESHOLD" | bc -l) )); then
          echo "‚ùå Coverage ${{ env.OVERALL_LINE_COV }}% is below threshold of ${THRESHOLD}%"
          echo "Consider adding more tests to improve coverage"
          # Uncomment the next line to fail the build on low coverage
          # exit 1
        else
          echo "‚úÖ Coverage ${{ env.OVERALL_LINE_COV }}% meets the threshold of ${THRESHOLD}%"
        fi

    - name: Upload test results and coverage
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: unit-test-results
        path: |
          tests/**/build/artifacts/test/
          tests/**/build/artifacts/gcov/
          tests/**/test_output.txt
          test_results.txt
          coverage-badge.json
        retention-days: 30

  # Metrics summary job
  metrics-summary:
    name: üìä Metrics Dashboard
    runs-on: ubuntu-latest
    needs: [astyle, cppcheck, drivers, unit-tests]
    if: always()

    steps:
    - name: Calculate Quality Score
      id: quality-score
      run: |
        # Calculate quality score based on job results - with safe arithmetic
        ASTYLE_SCORE=0
        CPPCHECK_SCORE=0
        DRIVERS_SCORE=0
        TESTS_SCORE=0

        # Check each job result safely
        if [ "${{ needs.astyle.result }}" = "success" ]; then
          ASTYLE_SCORE=25
        fi

        if [ "${{ needs.cppcheck.result }}" = "success" ]; then
          if [ "${{ env.STATIC_ANALYSIS_STRICT }}" = "true" ]; then
            # Strict mode: require zero issues for full score
            if [ "${{ needs.cppcheck.outputs.total_issues }}" = "0" ]; then
              CPPCHECK_SCORE=25
            fi
          else
            # Lenient mode: only require zero errors for full score
            if [ "${{ needs.cppcheck.outputs.has_errors }}" = "0" ]; then
              CPPCHECK_SCORE=25
            fi
          fi
        fi

        if [ "${{ needs.drivers.result }}" = "success" ]; then
          DRIVERS_SCORE=25
        fi

        if [ "${{ needs.unit-tests.result }}" = "success" ]; then
          TESTS_SCORE=25
        fi

        # Calculate total score safely
        TOTAL_SCORE=$((ASTYLE_SCORE + CPPCHECK_SCORE + DRIVERS_SCORE + TESTS_SCORE))

        echo "TOTAL_SCORE=$TOTAL_SCORE" >> $GITHUB_ENV

        # Determine overall status message
        if [ $TOTAL_SCORE -eq 100 ]; then
          echo "QUALITY_MESSAGE=üéâ **Excellent! All quality gates passed.**" >> $GITHUB_ENV
        elif [ $TOTAL_SCORE -ge 75 ]; then
          echo "QUALITY_MESSAGE=‚úÖ **Good quality, minor issues to address.**" >> $GITHUB_ENV
        elif [ $TOTAL_SCORE -ge 50 ]; then
          echo "QUALITY_MESSAGE=‚ö†Ô∏è **Quality improvements needed.**" >> $GITHUB_ENV
        else
          echo "QUALITY_MESSAGE=‚ùå **Significant quality issues require attention.**" >> $GITHUB_ENV
        fi

    - name: Calculate Static Analysis Status
      run: |
        echo "Debug: cppcheck.result=${{ needs.cppcheck.result }}"
        echo "Debug: STATIC_ANALYSIS_STRICT=${{ env.STATIC_ANALYSIS_STRICT }}"
        echo "Debug: total_issues=${{ needs.cppcheck.outputs.total_issues }}"
        echo "Debug: has_errors=${{ needs.cppcheck.outputs.has_errors }}"

        if [ "${{ needs.cppcheck.result }}" = "success" ]; then
          if [ "${{ env.STATIC_ANALYSIS_STRICT }}" = "true" ]; then
            # Strict mode: require zero issues
            TOTAL_ISSUES=$((0 + ${{ needs.cppcheck.outputs.total_issues }}))
            if [ $TOTAL_ISSUES -eq 0 ]; then
              echo "STATIC_ANALYSIS_STATUS=‚úÖ PASS" >> $GITHUB_ENV
            else
              echo "STATIC_ANALYSIS_STATUS=‚ùå FAIL" >> $GITHUB_ENV
            fi
          else
            # Lenient mode: only require zero errors (warnings/style/performance OK)
            ERRORS=$((0 + ${{ needs.cppcheck.outputs.has_errors }}))
            if [ $ERRORS -eq 0 ]; then
              echo "STATIC_ANALYSIS_STATUS=‚úÖ PASS" >> $GITHUB_ENV
            else
              echo "STATIC_ANALYSIS_STATUS=‚ùå FAIL" >> $GITHUB_ENV
            fi
          fi
        else
          echo "STATIC_ANALYSIS_STATUS=‚ùå FAIL" >> $GITHUB_ENV
        fi

    - name: Create Overall Dashboard
      run: |
        cat >> $GITHUB_STEP_SUMMARY << EOF
        # [üìä Overall Project Health Dashboard](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

        ## üéØ Quality Gates

        | Gate | Status | Details |
        |------|--------|---------|
        | üé® Code Style | ${{ needs.astyle.result == 'success' && '‚úÖ PASS' || '‚ùå FAIL' }} | Automated formatting compliance |
        | üîç Static Analysis | ${{ env.STATIC_ANALYSIS_STATUS }} | Code quality checks (${{ env.STATIC_ANALYSIS_STRICT == 'true' && 'STRICT' || 'LENIENT' }}) |
        | üî® Build | ${{ needs.drivers.result == 'success' && '‚úÖ PASS' || '‚ùå FAIL' }} | Compilation success |
        | üß™ Tests | ${{ needs.unit-tests.result == 'success' && '‚úÖ PASS' || '‚ùå FAIL' }} | Unit test execution |

        ## üìà Trend Indicators

        > üí° **Tip**: Click on individual job summaries above for detailed metrics
        > üîó **Dashboard**: Click the main heading to view the workflow run page

        ### üèÜ Quality Score
        ${{ env.TOTAL_SCORE }}/100

        ${{ env.QUALITY_MESSAGE }}
        EOF

  # Documentation with metrics
  documentation:
    name: Documentation
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 50
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install documentation dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y doxygen graphviz
        pip install sphinx sphinx-rtd-theme breathe

    - name: Set environment variables
      run: |
        echo "BUILD_TYPE=documentation" >> $GITHUB_ENV
        echo "UPDATE_GH_DOCS=${{ github.ref == 'refs/heads/main' && '1' || '0' }}" >> $GITHUB_ENV
        echo "GITHUB_DOC_TOKEN=${{ secrets.GITHUB_TOKEN }}" >> $GITHUB_ENV
        echo "REPO_SLUG=${{ github.repository }}" >> $GITHUB_ENV
        echo "BUILD_SOURCEBRANCH=${{ github.ref }}" >> $GITHUB_ENV

    - name: Build documentation with metrics
      run: |
        START_TIME=$(date +%s)

        # Capture documentation build output with detailed logging
        echo "=== Starting documentation build ===" > doc_output.txt
        echo "Build started at: $(date)" >> doc_output.txt

        # Run the build and capture result
        if ./ci/run_build.sh >> doc_output.txt 2>&1; then
          DOC_RESULT=0
          echo "DOC_STATUS=SUCCESS" >> doc_output.txt
          echo "Documentation build completed successfully" >> doc_output.txt
        else
          DOC_RESULT=1
          echo "DOC_STATUS=FAILED" >> doc_output.txt
          echo "Documentation build failed with errors" >> doc_output.txt
        fi

        END_TIME=$(date +%s)
        DOC_BUILD_TIME=$((END_TIME - START_TIME))

        # Count documentation files with multiple possible paths
        DOXYGEN_FILES=0
        SPHINX_FILES=0

        # Check various possible Doxygen output locations
        for doxy_path in "doc/doxygen/html" "doc/doxygen/build/html" "build/doc/doxygen/html"; do
          if [ -d "$doxy_path" ]; then
            DOXYGEN_COUNT=$(find "$doxy_path" -name "*.html" 2>/dev/null | wc -l || echo "0")
            DOXYGEN_FILES=$((DOXYGEN_FILES + DOXYGEN_COUNT))
            echo "Found $DOXYGEN_COUNT Doxygen files in $doxy_path" >> doc_output.txt
          fi
        done

        # Check various possible Sphinx output locations
        for sphinx_path in "doc/sphinx/_build" "doc/sphinx/_build/html" "doc/sphinx/build/html" "build/doc/sphinx/html"; do
          if [ -d "$sphinx_path" ]; then
            SPHINX_COUNT=$(find "$sphinx_path" -name "*.html" 2>/dev/null | wc -l || echo "0")
            SPHINX_FILES=$((SPHINX_FILES + SPHINX_COUNT))
            echo "Found $SPHINX_COUNT Sphinx files in $sphinx_path" >> doc_output.txt
          fi
        done

        # Calculate total pages
        TOTAL_PAGES=$((DOXYGEN_FILES + SPHINX_FILES))

        echo "Build completed at: $(date)" >> doc_output.txt
        echo "Total build time: ${DOC_BUILD_TIME}s" >> doc_output.txt
        echo "Doxygen pages: $DOXYGEN_FILES" >> doc_output.txt
        echo "Sphinx pages: $SPHINX_FILES" >> doc_output.txt
        echo "Total pages: $TOTAL_PAGES" >> doc_output.txt

        echo "DOC_BUILD_TIME=$DOC_BUILD_TIME" >> $GITHUB_ENV
        echo "DOC_RESULT=$DOC_RESULT" >> $GITHUB_ENV
        echo "DOXYGEN_FILES=$DOXYGEN_FILES" >> $GITHUB_ENV
        echo "SPHINX_FILES=$SPHINX_FILES" >> $GITHUB_ENV
        echo "TOTAL_PAGES=$TOTAL_PAGES" >> $GITHUB_ENV

    - name: Create Job Summary
      if: always()
      run: |
        cat >> $GITHUB_STEP_SUMMARY << EOF
        ## [üìö Documentation Build Results](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

        | Metric | Value |
        |--------|-------|
        | ‚è±Ô∏è Build Time | ${{ env.DOC_BUILD_TIME }}s |
        | üìÑ Doxygen Pages | ${{ env.DOXYGEN_FILES }} |
        | üìñ Sphinx Pages | ${{ env.SPHINX_FILES }} |
        | üìä Total Pages | ${{ env.TOTAL_PAGES }} |

        ### Status: ${{ env.DOC_RESULT == '0' && '‚úÖ Documentation Built Successfully' || '‚ùå Documentation Build Failed' }}

        ${{ env.UPDATE_GH_DOCS == '1' && 'üöÄ **Documentation will be deployed to GitHub Pages**' || 'üìù **Documentation built for preview**' }}
        EOF
