name: CI Enhanced with Metrics

on:
  push:
    branches:
      - main
      - staging/*
      - next_stable
      - '20*'
  pull_request:
    branches:
      - main
      - next_stable
      - '20*'

env:
  NUM_JOBS: 4

jobs:
  # Code style checking with astyle
  astyle:
    name: Code Style Check
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 50
        ref: ${{ github.event.pull_request.head.sha || github.sha }}

    - name: Fetch target branch
      run: |
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          git fetch --depth=50 origin ${{ github.event.pull_request.base.ref }}
        else
          git fetch --depth=50 origin main
        fi

    - name: Install astyle
      run: |
        sudo apt-get update
        sudo apt-get install -y astyle

    - name: Set environment variables
      run: |
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          echo "TARGET_BRANCH=${{ github.event.pull_request.base.ref }}" >> $GITHUB_ENV
        else
          echo "TARGET_BRANCH=main" >> $GITHUB_ENV
        fi
        echo "BUILD_TYPE=astyle" >> $GITHUB_ENV

    - name: Run astyle check
      run: |
        # Capture astyle output and git state
        echo "=== Running astyle check ===" > astyle_output.txt

        # Count source files before running astyle
        TOTAL_SOURCE_FILES=$(find . -name "*.c" -o -name "*.h" 2>/dev/null | grep -v ".git" | grep -v "build/" | wc -l)
        echo "Total source files found: $TOTAL_SOURCE_FILES" >> astyle_output.txt

        # Run astyle and capture detailed output
        if ./ci/run_build.sh >> astyle_output.txt 2>&1; then
          echo "astyle_failed=false" >> $GITHUB_ENV
          echo "Astyle check PASSED" >> astyle_output.txt
        else
          echo "astyle_failed=true" >> $GITHUB_ENV
          echo "Astyle check FAILED" >> astyle_output.txt
        fi

        # Check for files that were modified by astyle
        MODIFIED_FILES=$(git diff --name-only 2>/dev/null | grep -E "\.(c|h)$" | wc -l || echo "0")
        echo "Files modified by astyle: $MODIFIED_FILES" >> astyle_output.txt

        # Show some example changes if any
        if [ "$MODIFIED_FILES" -gt 0 ]; then
          echo "=== Modified files ===" >> astyle_output.txt
          git diff --name-only | grep -E "\.(c|h)$" >> astyle_output.txt 2>/dev/null || true
        fi

    - name: Parse astyle results
      run: |
        # Parse astyle results from our detailed output
        if [ -f "astyle_output.txt" ]; then
          # Get total source files
          TOTAL_FILES=$(grep "Total source files found:" astyle_output.txt | grep -o '[0-9]*' || echo "0")

          # Get files modified by astyle
          FILES_WITH_ISSUES=$(grep "Files modified by astyle:" astyle_output.txt | grep -o '[0-9]*' || echo "0")

          # Fallback: count source files if not captured
          if [ "$TOTAL_FILES" = "0" ] || [ -z "$TOTAL_FILES" ]; then
            TOTAL_FILES=$(find . -name "*.c" -o -name "*.h" 2>/dev/null | grep -v ".git" | grep -v "build/" | wc -l || echo "0")
          fi

          # Fallback: check git diff directly
          if [ "$FILES_WITH_ISSUES" = "0" ] || [ -z "$FILES_WITH_ISSUES" ]; then
            FILES_WITH_ISSUES=$(git diff --name-only 2>/dev/null | grep -E "\.(c|h)$" | wc -l || echo "0")
          fi
        else
          # If no output file, count files manually
          TOTAL_FILES=$(find . -name "*.c" -o -name "*.h" 2>/dev/null | grep -v ".git" | grep -v "build/" | wc -l || echo "0")
          FILES_WITH_ISSUES=$(git diff --name-only 2>/dev/null | grep -E "\.(c|h)$" | wc -l || echo "0")
        fi

        # Clean the variables
        TOTAL_FILES=$(echo "$TOTAL_FILES" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")
        FILES_WITH_ISSUES=$(echo "$FILES_WITH_ISSUES" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")

        # Ensure valid numbers
        TOTAL_FILES=${TOTAL_FILES:-0}
        FILES_WITH_ISSUES=${FILES_WITH_ISSUES:-0}

        # Validate numbers are reasonable
        if [ "$TOTAL_FILES" -lt 0 ] || [ "$TOTAL_FILES" -gt 100000 ]; then
          TOTAL_FILES=0
        fi
        if [ "$FILES_WITH_ISSUES" -lt 0 ] || [ "$FILES_WITH_ISSUES" -gt "$TOTAL_FILES" ]; then
          FILES_WITH_ISSUES=0
        fi

        # Calculate files OK
        FILES_OK=$((TOTAL_FILES - FILES_WITH_ISSUES))

        echo "TOTAL_FILES=$TOTAL_FILES" >> $GITHUB_ENV
        echo "FILES_WITH_ISSUES=$FILES_WITH_ISSUES" >> $GITHUB_ENV
        echo "FILES_OK=$FILES_OK" >> $GITHUB_ENV

        # Debug output
        echo "Debug: TOTAL_FILES=$TOTAL_FILES, FILES_WITH_ISSUES=$FILES_WITH_ISSUES, FILES_OK=$FILES_OK"

    - name: Create Job Summary
      if: always()
      run: |
        # Calculate compliance rate safely
        TOTAL_FILES="${{ env.TOTAL_FILES }}"
        FILES_OK="${{ env.FILES_OK }}"
        if [ "${TOTAL_FILES:-0}" -eq 0 ]; then
          COMPLIANCE_RATE="N/A"
        else
          COMPLIANCE_RATE=$(( FILES_OK * 100 / TOTAL_FILES ))
        fi

        cat >> $GITHUB_STEP_SUMMARY << EOF
        ## ðŸŽ¨ Code Style Check Results

        | Metric | Value |
        |--------|-------|
        | ðŸ“ Total Files Checked | ${{ env.TOTAL_FILES }} |
        | âœ… Files Compliant | ${{ env.FILES_OK }} |
        | âŒ Files with Issues | ${{ env.FILES_WITH_ISSUES }} |
        | ðŸ“Š Compliance Rate | ${COMPLIANCE_RATE}% |

        ### Status: ${{ env.astyle_failed == 'true' && 'âŒ Failed' || 'âœ… Passed' }}
        EOF

  # Static analysis with cppcheck
  cppcheck:
    name: Static Analysis
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 50
        ref: ${{ github.event.pull_request.head.sha || github.sha }}

    - name: Fetch target branch
      run: |
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          git fetch --depth=50 origin ${{ github.event.pull_request.base.ref }}
        else
          git fetch --depth=50 origin main
        fi

    - name: Install cppcheck
      run: |
        sudo apt-get update
        sudo apt-get install -y cppcheck

    - name: Set environment variables
      run: |
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          echo "TARGET_BRANCH=${{ github.event.pull_request.base.ref }}" >> $GITHUB_ENV
        else
          echo "TARGET_BRANCH=main" >> $GITHUB_ENV
        fi
        echo "BUILD_TYPE=cppcheck" >> $GITHUB_ENV

    - name: Run cppcheck analysis
      run: |
        ./ci/run_build.sh > cppcheck_output.txt 2>&1 || echo "cppcheck_failed=true" >> $GITHUB_ENV

    - name: Parse cppcheck results
      run: |
        # Parse different types of issues with safe parsing
        if [ -f "cppcheck_output.txt" ]; then
          ERRORS=$(grep -c ": error:" cppcheck_output.txt 2>/dev/null || echo "0")
          WARNINGS=$(grep -c ": warning:" cppcheck_output.txt 2>/dev/null || echo "0")
          STYLE=$(grep -c ": style:" cppcheck_output.txt 2>/dev/null || echo "0")
          PERFORMANCE=$(grep -c ": performance:" cppcheck_output.txt 2>/dev/null || echo "0")
        else
          ERRORS="0"
          WARNINGS="0"
          STYLE="0"
          PERFORMANCE="0"
        fi

        # Clean the variables of any whitespace/newlines
        ERRORS=$(echo "$ERRORS" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")
        WARNINGS=$(echo "$WARNINGS" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")
        STYLE=$(echo "$STYLE" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")
        PERFORMANCE=$(echo "$PERFORMANCE" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")

        # Ensure we have valid numbers
        ERRORS=${ERRORS:-0}
        WARNINGS=${WARNINGS:-0}
        STYLE=${STYLE:-0}
        PERFORMANCE=${PERFORMANCE:-0}

        # Validate numbers are reasonable
        if [ "$ERRORS" -lt 0 ] || [ "$ERRORS" -gt 10000 ]; then
          ERRORS=0
        fi
        if [ "$WARNINGS" -lt 0 ] || [ "$WARNINGS" -gt 10000 ]; then
          WARNINGS=0
        fi
        if [ "$STYLE" -lt 0 ] || [ "$STYLE" -gt 10000 ]; then
          STYLE=0
        fi
        if [ "$PERFORMANCE" -lt 0 ] || [ "$PERFORMANCE" -gt 10000 ]; then
          PERFORMANCE=0
        fi

        # Calculate total safely
        TOTAL_ISSUES=$((ERRORS + WARNINGS + STYLE + PERFORMANCE))

        echo "CPPCHECK_ERRORS=$ERRORS" >> $GITHUB_ENV
        echo "CPPCHECK_WARNINGS=$WARNINGS" >> $GITHUB_ENV
        echo "CPPCHECK_STYLE=$STYLE" >> $GITHUB_ENV
        echo "CPPCHECK_PERFORMANCE=$PERFORMANCE" >> $GITHUB_ENV
        echo "CPPCHECK_TOTAL=$TOTAL_ISSUES" >> $GITHUB_ENV

        # Debug output
        echo "Debug: ERRORS=$ERRORS, WARNINGS=$WARNINGS, STYLE=$STYLE, PERFORMANCE=$PERFORMANCE, TOTAL=$TOTAL_ISSUES"

    - name: Create Job Summary
      if: always()
      run: |
        # Calculate status message
        if [ "${{ env.cppcheck_failed }}" = "true" ]; then
          STATUS_MSG="âŒ Failed"
        elif [ "${{ env.CPPCHECK_ERRORS }}" -gt 0 ]; then
          STATUS_MSG="âš ï¸ Has Errors"
        elif [ "${{ env.CPPCHECK_TOTAL }}" -gt 0 ]; then
          STATUS_MSG="âš ï¸ Has Warnings"
        else
          STATUS_MSG="âœ… Passed"
        fi

        cat >> $GITHUB_STEP_SUMMARY << EOF
        ## ðŸ” Static Analysis Results

        | Issue Type | Count |
        |------------|-------|
        | ðŸš¨ Errors | ${{ env.CPPCHECK_ERRORS }} |
        | âš ï¸ Warnings | ${{ env.CPPCHECK_WARNINGS }} |
        | ðŸŽ¯ Style Issues | ${{ env.CPPCHECK_STYLE }} |
        | âš¡ Performance Issues | ${{ env.CPPCHECK_PERFORMANCE }} |
        | **ðŸ“Š Total Issues** | **${{ env.CPPCHECK_TOTAL }}** |

        ### Status: ${STATUS_MSG}
        EOF

        # Add issue breakdown if there are issues
        if [ "${{ env.CPPCHECK_TOTAL }}" -gt 0 ]; then
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'

        #### ðŸ“‹ Issue Breakdown

        - **Errors**: High priority issues that should be fixed immediately
        - **Warnings**: Potential problems worth investigating
        - **Style**: Code style improvements for better readability
        - **Performance**: Optimization suggestions for better efficiency
        EOF
        else
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'

        ðŸŽ‰ **No issues found!** Your code passes all static analysis checks.
        EOF
        fi

  # Build drivers with metrics
  drivers:
    name: Build Drivers
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 50

    - name: Install build dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential gcc-arm-none-eabi

    - name: Set environment variables
      run: |
        echo "BUILD_TYPE=drivers" >> $GITHUB_ENV

    - name: Build drivers with metrics
      run: |
        START_TIME=$(date +%s)

        # Capture build output with detailed logging
        echo "=== Starting drivers build ===" > build_output.txt
        echo "Build started at: $(date)" >> build_output.txt

        # Count actual driver source files that will be compiled
        # Use the same logic as the drivers Makefile
        cd drivers
        SKIPDIR="-path ./platform -prune -o -path ./axi_core -prune -o -path ./rf-transceiver/navassa -prune -o -path ./adc/ad9081 -prune -o -path ./rf-transceiver/madura -prune -o -path ./net/oa_tc6 -prune -o"
        TOTAL_DRIVERS=$(find . $SKIPDIR -name '*.c' -print | wc -l || echo "0")
        cd ..
        
        echo "Total driver files to compile: $TOTAL_DRIVERS" >> build_output.txt
        echo "This matches the drivers/Makefile SRCS variable" >> build_output.txt

        # Run the build and capture result
        if ./ci/run_build.sh >> build_output.txt 2>&1; then
          BUILD_RESULT=0
          echo "BUILD_STATUS=SUCCESS" >> build_output.txt
          echo "All driver compilation completed successfully" >> build_output.txt
        else
          BUILD_RESULT=1
          echo "BUILD_STATUS=FAILED" >> build_output.txt
          echo "Driver compilation failed with errors" >> build_output.txt
        fi

        END_TIME=$(date +%s)
        BUILD_TIME=$((END_TIME - START_TIME))

        echo "Build completed at: $(date)" >> build_output.txt
        echo "Total build time: ${BUILD_TIME}s" >> build_output.txt
        echo "Driver files processed: $TOTAL_DRIVERS" >> build_output.txt

        echo "BUILD_TIME=$BUILD_TIME" >> $GITHUB_ENV
        echo "BUILD_RESULT=$BUILD_RESULT" >> $GITHUB_ENV

    - name: Parse build results
      run: |
        # Parse build results from our detailed output
        if [ -f "build_output.txt" ]; then
          # Get total driver files that were compiled
          TOTAL_DRIVERS=$(grep "Total driver files to compile:" build_output.txt | grep -o '[0-9]*' || echo "0")

          # Get build status
          BUILD_STATUS=$(grep "BUILD_STATUS=" build_output.txt | cut -d'=' -f2 || echo "UNKNOWN")

          # Determine successful vs failed builds based on overall result
          if [ "$BUILD_STATUS" = "SUCCESS" ] || [ "${{ env.BUILD_RESULT }}" = "0" ]; then
            SUCCESSFUL_BUILDS="$TOTAL_DRIVERS"
            FAILED_BUILDS="0"
          elif [ "$BUILD_STATUS" = "FAILED" ] || [ "${{ env.BUILD_RESULT }}" != "0" ]; then
            SUCCESSFUL_BUILDS="0"
            FAILED_BUILDS="$TOTAL_DRIVERS"
          else
            # Unknown status - be conservative
            SUCCESSFUL_BUILDS="0"
            FAILED_BUILDS="0"
          fi

          # Fallback: count driver files if not captured
          if [ "$TOTAL_DRIVERS" = "0" ] || [ -z "$TOTAL_DRIVERS" ]; then
            cd drivers 2>/dev/null || cd .
            SKIPDIR="-path ./platform -prune -o -path ./axi_core -prune -o -path ./rf-transceiver/navassa -prune -o -path ./adc/ad9081 -prune -o -path ./rf-transceiver/madura -prune -o -path ./net/oa_tc6 -prune -o"
            TOTAL_DRIVERS=$(find . $SKIPDIR -name '*.c' -print 2>/dev/null | wc -l || echo "0")
            cd .. 2>/dev/null || true
          fi
        else
          # If no output file, count driver files manually
          cd drivers 2>/dev/null || cd .
          SKIPDIR="-path ./platform -prune -o -path ./axi_core -prune -o -path ./rf-transceiver/navassa -prune -o -path ./adc/ad9081 -prune -o -path ./rf-transceiver/madura -prune -o -path ./net/oa_tc6 -prune -o"
          TOTAL_DRIVERS=$(find . $SKIPDIR -name '*.c' -print 2>/dev/null | wc -l || echo "0")
          cd .. 2>/dev/null || true

          # Use build result from environment
          if [ "${{ env.BUILD_RESULT }}" = "0" ]; then
            SUCCESSFUL_BUILDS="$TOTAL_DRIVERS"
            FAILED_BUILDS="0"
          else
            SUCCESSFUL_BUILDS="0"
            FAILED_BUILDS="$TOTAL_DRIVERS"
          fi
        fi

        # Clean the variables
        TOTAL_DRIVERS=$(echo "$TOTAL_DRIVERS" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")
        SUCCESSFUL_BUILDS=$(echo "$SUCCESSFUL_BUILDS" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")
        FAILED_BUILDS=$(echo "$FAILED_BUILDS" | tr -d '\n\r' | grep -o '^[0-9]*$' || echo "0")

        # Ensure valid numbers
        TOTAL_DRIVERS=${TOTAL_DRIVERS:-0}
        SUCCESSFUL_BUILDS=${SUCCESSFUL_BUILDS:-0}
        FAILED_BUILDS=${FAILED_BUILDS:-0}

        # Validate numbers are reasonable
        if [ "$TOTAL_DRIVERS" -lt 0 ] || [ "$TOTAL_DRIVERS" -gt 1000 ]; then
          TOTAL_DRIVERS=0
        fi
        if [ "$SUCCESSFUL_BUILDS" -lt 0 ] || [ "$SUCCESSFUL_BUILDS" -gt "$TOTAL_DRIVERS" ]; then
          SUCCESSFUL_BUILDS=0
        fi
        if [ "$FAILED_BUILDS" -lt 0 ] || [ "$FAILED_BUILDS" -gt "$TOTAL_DRIVERS" ]; then
          FAILED_BUILDS=0
        fi

        echo "SUCCESSFUL_BUILDS=$SUCCESSFUL_BUILDS" >> $GITHUB_ENV
        echo "FAILED_BUILDS=$FAILED_BUILDS" >> $GITHUB_ENV
        echo "TOTAL_DRIVERS=$TOTAL_DRIVERS" >> $GITHUB_ENV

        # Debug output
        echo "Debug: TOTAL_DRIVERS=$TOTAL_DRIVERS, SUCCESSFUL_BUILDS=$SUCCESSFUL_BUILDS, FAILED_BUILDS=$FAILED_BUILDS, BUILD_STATUS=$BUILD_STATUS"

    - name: Create Job Summary
      if: always()
      run: |
        # Calculate success rate safely
        TOTAL_DRIVERS="${{ env.TOTAL_DRIVERS }}"
        SUCCESSFUL_BUILDS="${{ env.SUCCESSFUL_BUILDS }}"
        if [ "${TOTAL_DRIVERS:-0}" -eq 0 ]; then
          SUCCESS_RATE="N/A"
        else
          SUCCESS_RATE=$(( SUCCESSFUL_BUILDS * 100 / TOTAL_DRIVERS ))
        fi

        cat >> $GITHUB_STEP_SUMMARY << EOF
        ## ðŸ”¨ Drivers Build Results

        ### ðŸ“Š Build Metrics
        | Metric | Value |
        |--------|-------|
        | â±ï¸ Build Time | ${{ env.BUILD_TIME }}s |
        | ðŸŽ¯ Total Driver Files | ${{ env.TOTAL_DRIVERS }} |
        | âœ… Successfully Compiled | ${{ env.SUCCESSFUL_BUILDS }} |
        | âŒ Failed to Compile | ${{ env.FAILED_BUILDS }} |
        | ðŸ“ˆ Success Rate | ${SUCCESS_RATE}% |

        ### Status: ${{ env.BUILD_RESULT == '0' && 'âœ… All Drivers Compiled Successfully' || 'âŒ Driver Compilation Failed' }}

        ${{ env.BUILD_TIME > 300 && 'âš ï¸ **Build time exceeded 5 minutes. Consider optimization.**' || '' }}
        
        > ðŸ’¡ **Note**: This builds reusable driver components (${TOTAL_DRIVERS} .c files), not the 134 application projects in the \`projects/\` directory.
        EOF

  # Unit tests with coverage metrics
  unit-tests:
    name: Unit Tests & Coverage
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 50

    - name: Setup Ruby
      uses: ruby/setup-ruby@v1
      with:
        ruby-version: '3.0'
        bundler-cache: false

    - name: Install Ceedling and dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential gcc lcov python3-pip
        pip3 install gcovr
        gem install ceedling

    - name: Run unit tests and generate coverage
      run: |
        set -e

        # Initialize counters
        TOTAL_TESTS=0
        PASSED_TESTS=0
        FAILED_TESTS=0
        TOTAL_COVERAGE=0
        TEST_DIRS_COUNT=0
        
        # Create detailed test results file
        echo "=== Unit Test Results ===" > test_results.txt
        echo "Run started at: $(date)" >> test_results.txt
        echo "" >> test_results.txt

        # Find all test directories containing project.yml files
        test_dirs=$(find tests -name "project.yml" -type f -exec dirname {} \;)

        if [ -z "$test_dirs" ]; then
          echo "No test directories found with project.yml files"
          exit 1
        fi

        for test_dir in $test_dirs; do
          echo "=== Running tests in: $test_dir ===" >> test_results.txt
          echo "Running tests in: $test_dir"
          cd "$test_dir"

          # Run tests with coverage
          if ceedling gcov:all > test_output.txt 2>&1; then
            echo "Test suite completed successfully" >> ../../../test_results.txt
          else
            echo "Test suite encountered errors" >> ../../../test_results.txt
          fi

          # Parse test results from Ceedling output
          # Look for the OVERALL TEST SUMMARY section
          DIR_TESTS=$(grep "^TESTED:" test_output.txt 2>/dev/null | tail -1 | grep -o '[0-9]\+' || echo "0")
          DIR_PASSED=$(grep "^PASSED:" test_output.txt 2>/dev/null | tail -1 | grep -o '[0-9]\+' || echo "0")
          DIR_FAILED=$(grep "^FAILED:" test_output.txt 2>/dev/null | tail -1 | grep -o '[0-9]\+' || echo "0")
          
          # Validate the numbers
          DIR_TESTS=${DIR_TESTS:-0}
          DIR_PASSED=${DIR_PASSED:-0}  
          DIR_FAILED=${DIR_FAILED:-0}
          
          # Capture failed test names
          if [ "$DIR_FAILED" -gt 0 ]; then
            echo "âŒ Failed tests in $test_dir:" >> ../../../test_results.txt
            # Look for test failure details in Ceedling output
            grep -A 5 -B 2 "FAIL\|ERROR" test_output.txt | head -20 >> ../../../test_results.txt 2>/dev/null || echo "  (Failed test details not available)" >> ../../../test_results.txt
            echo "" >> ../../../test_results.txt
          else
            echo "âœ… All tests passed in $test_dir" >> ../../../test_results.txt
          fi
          
          # Capture passed test summary
          if [ "$DIR_PASSED" -gt 0 ]; then
            echo "âœ… Passed: $DIR_PASSED tests" >> ../../../test_results.txt
          fi
          
          echo "Total tests in $test_dir: $DIR_TESTS (Passed: $DIR_PASSED, Failed: $DIR_FAILED)" >> ../../../test_results.txt
          echo "" >> ../../../test_results.txt

          TOTAL_TESTS=$((TOTAL_TESTS + DIR_TESTS))
          PASSED_TESTS=$((PASSED_TESTS + DIR_PASSED))
          FAILED_TESTS=$((FAILED_TESTS + DIR_FAILED))
          TEST_DIRS_COUNT=$((TEST_DIRS_COUNT + 1))

          cd - > /dev/null
        done

        # Add summary to results
        echo "=== Test Summary ===" >> test_results.txt
        echo "Total test suites: $TEST_DIRS_COUNT" >> test_results.txt
        echo "Total tests: $TOTAL_TESTS" >> test_results.txt
        echo "Passed: $PASSED_TESTS" >> test_results.txt
        echo "Failed: $FAILED_TESTS" >> test_results.txt

        # Calculate overall coverage (simplified)
        if [ $TOTAL_TESTS -gt 0 ]; then
          TEST_SUCCESS_RATE=$((PASSED_TESTS * 100 / TOTAL_TESTS))
        else
          TEST_SUCCESS_RATE=0
        fi

        echo "TOTAL_TESTS=$TOTAL_TESTS" >> $GITHUB_ENV
        echo "PASSED_TESTS=$PASSED_TESTS" >> $GITHUB_ENV
        echo "FAILED_TESTS=$FAILED_TESTS" >> $GITHUB_ENV
        echo "TEST_SUCCESS_RATE=$TEST_SUCCESS_RATE" >> $GITHUB_ENV
        echo "TEST_DIRS_COUNT=$TEST_DIRS_COUNT" >> $GITHUB_ENV

    - name: Create Job Summary
      if: always()
      run: |
        cat >> $GITHUB_STEP_SUMMARY << 'EOF'
        ## ðŸ§ª Unit Tests & Coverage Dashboard

        ### ðŸ“Š Test Metrics
        | Metric | Value |
        |--------|-------|
        | ðŸŽ¯ Test Suites | ${{ env.TEST_DIRS_COUNT }} |
        | âœ… Total Tests | ${{ env.TOTAL_TESTS }} |
        | ðŸŸ¢ Passed | ${{ env.PASSED_TESTS }} |
        | ðŸ”´ Failed | ${{ env.FAILED_TESTS }} |
        | ðŸ“ˆ Success Rate | ${{ env.TEST_SUCCESS_RATE }}% |

        ### Status: ${{ env.FAILED_TESTS == '0' && 'âœ… All Tests Passed' || 'âŒ Test Failures Detected' }}

        ${{ env.TEST_SUCCESS_RATE < 90 && 'âš ï¸ **Test success rate below 90%. Consider improving test reliability.**' || '' }}
        ${{ env.TOTAL_TESTS == '0' && 'âš ï¸ **No tests found. Consider adding unit tests.**' || '' }}
        EOF
        
        # Add detailed test results if there are failures
        if [ "${{ env.FAILED_TESTS }}" != "0" ] && [ -f "test_results.txt" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ” Detailed Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          # Show failed tests and summary (limit output to prevent overflow)
          grep -A 20 "Failed tests" test_results.txt | head -50 >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "Test failure details not available" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "> ðŸ“ **Full test results are available in the uploaded artifacts**" >> $GITHUB_STEP_SUMMARY
        fi

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: unit-test-results
        path: |
          tests/**/build/artifacts/test/
          tests/**/build/artifacts/gcov/
          tests/**/test_output.txt
          test_results.txt
        retention-days: 30

  # Metrics summary job
  metrics-summary:
    name: ðŸ“Š Metrics Dashboard
    runs-on: ubuntu-latest
    needs: [astyle, cppcheck, drivers, unit-tests]
    if: always()

    steps:
    - name: Calculate Quality Score
      id: quality-score
      run: |
        # Calculate quality score based on job results - with safe arithmetic
        ASTYLE_SCORE=0
        CPPCHECK_SCORE=0
        DRIVERS_SCORE=0
        TESTS_SCORE=0

        # Check each job result safely
        if [ "${{ needs.astyle.result }}" = "success" ]; then
          ASTYLE_SCORE=25
        fi

        if [ "${{ needs.cppcheck.result }}" = "success" ]; then
          CPPCHECK_SCORE=25
        fi

        if [ "${{ needs.drivers.result }}" = "success" ]; then
          DRIVERS_SCORE=25
        fi

        if [ "${{ needs.unit-tests.result }}" = "success" ]; then
          TESTS_SCORE=25
        fi

        # Calculate total score safely
        TOTAL_SCORE=$((ASTYLE_SCORE + CPPCHECK_SCORE + DRIVERS_SCORE + TESTS_SCORE))

        echo "TOTAL_SCORE=$TOTAL_SCORE" >> $GITHUB_ENV

        # Determine overall status message
        if [ $TOTAL_SCORE -eq 100 ]; then
          echo "QUALITY_MESSAGE=ðŸŽ‰ **Excellent! All quality gates passed.**" >> $GITHUB_ENV
        elif [ $TOTAL_SCORE -ge 75 ]; then
          echo "QUALITY_MESSAGE=âœ… **Good quality, minor issues to address.**" >> $GITHUB_ENV
        else
          echo "QUALITY_MESSAGE=âš ï¸ **Quality improvements needed.**" >> $GITHUB_ENV
        fi

    - name: Create Overall Dashboard
      run: |
        cat >> $GITHUB_STEP_SUMMARY << 'EOF'
        # ðŸ“Š Overall Project Health Dashboard

        ## ðŸŽ¯ Quality Gates

        | Gate | Status | Details |
        |------|--------|---------|
        | ðŸŽ¨ Code Style | ${{ needs.astyle.result == 'success' && 'âœ… PASS' || 'âŒ FAIL' }} | Automated formatting compliance |
        | ðŸ” Static Analysis | ${{ needs.cppcheck.result == 'success' && 'âœ… PASS' || 'âŒ FAIL' }} | Code quality checks |
        | ðŸ”¨ Build | ${{ needs.drivers.result == 'success' && 'âœ… PASS' || 'âŒ FAIL' }} | Compilation success |
        | ðŸ§ª Tests | ${{ needs.unit-tests.result == 'success' && 'âœ… PASS' || 'âŒ FAIL' }} | Unit test execution |

        ## ðŸ“ˆ Trend Indicators

        > ðŸ’¡ **Tip**: Click on individual job summaries above for detailed metrics

        ### ðŸ† Quality Score
        ${{ env.TOTAL_SCORE }}/100

        ${{ env.QUALITY_MESSAGE }}
        EOF

  # Documentation with metrics
  documentation:
    name: Documentation
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 50
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install documentation dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y doxygen graphviz
        pip install sphinx sphinx-rtd-theme breathe

    - name: Set environment variables
      run: |
        echo "BUILD_TYPE=documentation" >> $GITHUB_ENV
        echo "UPDATE_GH_DOCS=${{ github.ref == 'refs/heads/main' && '1' || '0' }}" >> $GITHUB_ENV
        echo "GITHUB_DOC_TOKEN=${{ secrets.GITHUB_TOKEN }}" >> $GITHUB_ENV
        echo "REPO_SLUG=${{ github.repository }}" >> $GITHUB_ENV
        echo "BUILD_SOURCEBRANCH=${{ github.ref }}" >> $GITHUB_ENV

    - name: Build documentation with metrics
      run: |
        START_TIME=$(date +%s)

        # Capture documentation build output with detailed logging
        echo "=== Starting documentation build ===" > doc_output.txt
        echo "Build started at: $(date)" >> doc_output.txt

        # Run the build and capture result
        if ./ci/run_build.sh >> doc_output.txt 2>&1; then
          DOC_RESULT=0
          echo "DOC_STATUS=SUCCESS" >> doc_output.txt
          echo "Documentation build completed successfully" >> doc_output.txt
        else
          DOC_RESULT=1
          echo "DOC_STATUS=FAILED" >> doc_output.txt
          echo "Documentation build failed with errors" >> doc_output.txt
        fi

        END_TIME=$(date +%s)
        DOC_BUILD_TIME=$((END_TIME - START_TIME))

        # Count documentation files with multiple possible paths
        DOXYGEN_FILES=0
        SPHINX_FILES=0

        # Check various possible Doxygen output locations
        for doxy_path in "doc/doxygen/html" "doc/doxygen/build/html" "build/doc/doxygen/html"; do
          if [ -d "$doxy_path" ]; then
            DOXYGEN_COUNT=$(find "$doxy_path" -name "*.html" 2>/dev/null | wc -l || echo "0")
            DOXYGEN_FILES=$((DOXYGEN_FILES + DOXYGEN_COUNT))
            echo "Found $DOXYGEN_COUNT Doxygen files in $doxy_path" >> doc_output.txt
          fi
        done

        # Check various possible Sphinx output locations
        for sphinx_path in "doc/sphinx/_build" "doc/sphinx/_build/html" "doc/sphinx/build/html" "build/doc/sphinx/html"; do
          if [ -d "$sphinx_path" ]; then
            SPHINX_COUNT=$(find "$sphinx_path" -name "*.html" 2>/dev/null | wc -l || echo "0")
            SPHINX_FILES=$((SPHINX_FILES + SPHINX_COUNT))
            echo "Found $SPHINX_COUNT Sphinx files in $sphinx_path" >> doc_output.txt
          fi
        done

        # Calculate total pages
        TOTAL_PAGES=$((DOXYGEN_FILES + SPHINX_FILES))

        echo "Build completed at: $(date)" >> doc_output.txt
        echo "Total build time: ${DOC_BUILD_TIME}s" >> doc_output.txt
        echo "Doxygen pages: $DOXYGEN_FILES" >> doc_output.txt
        echo "Sphinx pages: $SPHINX_FILES" >> doc_output.txt
        echo "Total pages: $TOTAL_PAGES" >> doc_output.txt

        echo "DOC_BUILD_TIME=$DOC_BUILD_TIME" >> $GITHUB_ENV
        echo "DOC_RESULT=$DOC_RESULT" >> $GITHUB_ENV
        echo "DOXYGEN_FILES=$DOXYGEN_FILES" >> $GITHUB_ENV
        echo "SPHINX_FILES=$SPHINX_FILES" >> $GITHUB_ENV
        echo "TOTAL_PAGES=$TOTAL_PAGES" >> $GITHUB_ENV

    - name: Create Job Summary
      if: always()
      run: |
        cat >> $GITHUB_STEP_SUMMARY << 'EOF'
        ## ðŸ“š Documentation Build Results

        | Metric | Value |
        |--------|-------|
        | â±ï¸ Build Time | ${{ env.DOC_BUILD_TIME }}s |
        | ðŸ“„ Doxygen Pages | ${{ env.DOXYGEN_FILES }} |
        | ðŸ“– Sphinx Pages | ${{ env.SPHINX_FILES }} |
        | ðŸ“Š Total Pages | ${{ env.TOTAL_PAGES }} |

        ### Status: ${{ env.DOC_RESULT == '0' && 'âœ… Documentation Built Successfully' || 'âŒ Documentation Build Failed' }}

        ${{ env.UPDATE_GH_DOCS == '1' && 'ðŸš€ **Documentation will be deployed to GitHub Pages**' || 'ðŸ“ **Documentation built for preview**' }}
        EOF
